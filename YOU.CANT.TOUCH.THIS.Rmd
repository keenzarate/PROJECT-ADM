---
title: "YOU.CANT.TOUCH.THIS"
author: "Keen,Obinna, Tari, Roo"
date: "5/19/2019"
output:
  pdf_document: default
  html_document: default
---

Load packages here:
```{r}

suppressMessages(library(ggrepel))
suppressMessages(library(graphics))
suppressMessages(library(factoextra))
suppressMessages(library(lattice))
suppressMessages(library(dplyr))
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(MASS))
suppressMessages(library(tree))
suppressMessages(library(mosaic))
suppressMessages(library(randomForest))
suppressMessages(library(rpart))
library(tidyverse)
library(ggplot2)
library(lattice)
library(mosaic)
library(MASS)
library(tree)
library(randomForest)
library(dplyr)
library(ggrepel)
library(graphics)
library(factoextra)
library(gbm)
```

## Exploratory Data Analysis

Load the data. 
```{r}
#Lets take a look at the data
attrition = read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
#View(attrition)
```

What does the MonthlyIncome look like?
```{r}
#Check for missing values
ggplot(attrition, aes(x = MonthlyIncome)) + geom_histogram()  
```
Histogram shows the distribution of monthly income in our dataset. The histogram is left skewed therefore we see that monthly income. Seems like most have MonthlyIncome around 2000. 


```{r}

ggplot(attrition,aes(x=Age,y=MonthlyIncome,col=EducationField))+
  geom_jitter(size=2)+
  facet_grid(".~Gender")

#qplot(attrition$Age, log(attrition$MonthlyIncome), data = attrition, color = attrition$EducationField)
```
The graphs above shows a positive relationship between age and monthly income across different kinds of educational fields. There seem to be a lot of people who are in the EducationField = "Life Sciences". 

Here, we are curious to see how YearSinceLastPromotion and YearsWithCurrManager affect Attrition. 
```{r}
ggplot(attrition, 
        aes(y = YearsSinceLastPromotion, x = YearsWithCurrManager, colour = OverTime)) + 
        geom_jitter(size = 1, alpha = 0.9) + 
         geom_smooth(method = "gam") + 
        facet_wrap(~ Attrition) + 
        ggtitle("Attrition") + scale_color_brewer(palette="Dark2") + theme_minimal()+
        theme(plot.title = element_text(hjust = 0.2))

```

Observation: The graph above shows that people who are not getting a promotion and they've worked are quitting their job. Seems like the years witht current manager could be a factor of how likely they are to leave. 


Below is a graph on Attrition and NumCompaniesWorked. 
```{r}
#Attrition vs Number of companies worked
hase = ggplot(attrition, aes(x = Attrition, y = NumCompaniesWorked)) +
  geom_boxplot()
hase
```
The boxplot above shows that most employees lost to attrition have worked fewer companies than those who are not.

The graphs below suggests that the greater the distance from home the more likely it is for an employee to be lost through attrition.
```{r}
require(ggthemes)
ggplot(attrition,aes(DistanceFromHome,fill=Attrition))+
  geom_density(alpha=0.5)+
  theme_few()+
  theme(legend.position="bottom",plot.title=element_text(hjust=0.5,size=16))+
  labs(x="Distance from Home",title="Attrition and Distance from Home")+scale_fill_brewer(palette = "Set2")
```
Observation: Graph above shows that below who lives close are less likely to quite their job. 

Below are boxplots on Attrition based on DailyRate, HourlyRate, Monthly Rate, Monthly Income. 
```{r}
#Attition vs Daily Rate/Distance From Home/ Houtly Rate
old.par = par()
par(mfrow = c(1,4))
with (attrition, {
  boxplot(attrition$DailyRate ~ attrition$Attrition, xlab = "Attrition by Daily Rate")
  boxplot(attrition$HourlyRate ~ attrition$Attrition, xlab = "Attrition by Hourly Rate")
  boxplot(attrition$MonthlyRate ~ attrition$Attrition, xlab = "Attrition by Monthly Rate")
  boxplot(attrition$MonthlyIncome ~ attrition$Attrition, xlab = "Attrition by Monthly Income")
}
)
```

From the boxplots above, it seems attrition is most significantly affected by the daily rate as those who faced attrition had a significantly lower mean for daily rate as opposed to those who did not. As for attrition by hourly rate and by monthly rate, there does not seem to be any significant difference in means.


Below is graph on on Marital Status:
```{r}
ggplot(attrition, aes(x=MaritalStatus)) +
  ggtitle("MaritalStatus") +
  xlab("MaritalStatus") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5,col="blue",fill="orange") +
  ylab("Percentage") +
  theme_minimal()
```
The bar graph above shows that there is a lot more married people working than single and divorced. 

```{r}
#Job role vs percent salary hike
ggplot(attrition,aes(x=JobRole,y=PercentSalaryHike,col=JobRole))+geom_jitter(size=1)

Haha = favstats(~attrition$PercentSalaryHike|attrition$JobRole)

bwplot(attrition$PercentSalaryHike ~ attrition$JobRole, data=attrition)

#attrition vs percent salary hike
Hehe = favstats(~attrition$PercentSalaryHike|attrition$Attrition)

bwplot(attrition$PercentSalaryHike ~ attrition$Attrition, data=attrition)

#Gender by Montly Income
bwplot(attrition$MonthlyIncome ~ attrition$Gender, data=attrition)
```
Based on the tables above, Percent Salary Hike is independent of Job Role and is not a major factor in determining attrition as the means are almost the same between those employees lost to attrition and those not.

Looking into attrition and percentsalaryhike accross genders
```{r}
ggplot(attrition,aes(x=PercentSalaryHike,fill=Attrition))+
  geom_bar()+
  facet_grid(".~Gender")+scale_fill_brewer(palette = "Set2")
```
There's a lot more males that are having percentsalaryhike it seems like in this data set. 

How about attrition and monthlyIncome across gender?
```{r}
ggplot(attrition,aes(y=MonthlyIncome,x=Attrition,col=MonthlyIncome))+
  geom_jitter(size=2)+
  facet_grid(".~Gender") 
```

It is also interesting from the Monthly income by gender boxplot that both men and women seem to be earning roughly the same amount which is usually not the case.


How about attrition and EnvironmentSatisfaction across gender?
```{r}
ggplot(attrition,aes(x=EnvironmentSatisfaction,fill=Attrition))+
  geom_bar()+
  facet_grid(".~Gender")+scale_fill_brewer(palette = "Set2")
```
It seems like a lot of employees are pretty satisfied with their job or environment.


From the below scatterplot, we can see that there is no linear relationship between monthly income and monthly rate. There's a lot more people who are doing "good"=3 than "excellent"=4
```{r}
ggplot(attrition,aes(x=MonthlyRate,y=MonthlyIncome,col=PerformanceRating))+
  geom_jitter(size=2)+
  facet_grid(".~PerformanceRating")
```


##PCA

Now the question we are trying to answer is whether we can group employees based on their age. In order to do this we shall use PCA and kmeans clustering.

Before trying to cluster since there are a lot of variables lets try to reduce the number of variables through picking the most important ones, via subselection using random forest.

change order of columns. We do this so that age is on the end as it is what we are trying to predict.
```{r}
attri2 <- attrition[,c(2:34,1)]
#we get rid of over18 as the only output is Y for yes so it does not help us in anyway
attri2 <- attri2[,-c(21)]
```

We create a sample size for our random sampling via bootstrap
```{r}
sampleSize <- nrow(attrition)
```

This is our bootstrap function, its a form of random sampling, allowing us to cross validate our data and predictions using random forest
```{r}
mseBoot <- function(data.df,M=50){
    mse <- rep(0,M)
    for(m in 1:M){
        bootSamp <- sample(1:sampleSize,sampleSize,rep=TRUE)
        outOfBag <- setdiff(1:sampleSize,bootSamp)
        train.df <- data.df[bootSamp,]
        test.df <-   data.df[outOfBag,]
        mod <- glm(Age~ .,data=train.df)
        vals <- predict(mod,newdata=test.df)
        mse[m] <- with(test.df,mean((Age-vals)^2))
    }
    mean(mse)
}
```

Now we are setting up for our large loop that will do backward stepwise subset selection
```{r}
numPreds <- length(names(attri2))-1
```

```{r}
allPreds <- 1:(numPreds)
```

```{r}
currPreds <- c()
```

```{r}
availPreds <- setdiff(allPreds,currPreds)
length(availPreds)
```

```{r}
maxPreds <- numPreds
```

```{r}
minMSE <- numeric(maxPreds)
```

Okay everything is set up, to gothrough each of our predictors trying to fnd which variables give us the lowest MSE and saving that MSE in a list as well as the predictor used.

Alright here we go, this may take a bit

```{r warning=FALSE}
tot <- 0
while( tot < maxPreds){
    ##add predictor which decreases MSE (as determined by
    ##Bootstrapping)
    ## The MSEs computed as we add each of the available predictors
    allMSE <- numeric(length(availPreds))
    ctr<-1
    for(id in availPreds){
        data.df <- attri2[,c(currPreds,id,numPreds+1)]
        mse <- mseBoot(data.df,30)
        allMSE[ctr] <- mse
        ctr<-ctr+1
    }
    ##Find the min
    id <- which.min(allMSE)
    ##get the best predictor and MSW
    bestPred <- availPreds[id]
    bestMSE <- min(allMSE)
    ##Add these into the collection
    currPreds <- c(currPreds,bestPred)
    tot <-tot+1
    minMSE[tot] <- bestMSE
    availPreds <- setdiff(allPreds,currPreds)
    ## Print stuff out for debugging and attention-grabbing
    print(sprintf("Predictor Added: %s  MSE Value: %s",bestPred,bestMSE))
    print(currPreds)
}
```
 It works, so we can now look at MSE Plot

```{r}
data.frame(pred=1:maxPreds,minMSE) %>% 
    ggplot()+
    geom_point(aes(pred,minMSE))
```

We can see that we have our minimal MSE at around 8 predictors.

We are now going to try to generate a plot to better illustrate the importance of our predctors.

Lets take these names and put them in a list
```{r}
Titles <- names(attri2[,currPreds])
```
 
Here we are calculaing change in MSE
```{r}
diffMSE <- c(0,minMSE[-length(minMSE)]-minMSE[-1])
head(diffMSE)
```


Lets put everything in a results dataframe
```{r}
result.df <- data.frame(id=1:length(Titles),Titles,minMSE,diffMSE=-10*diffMSE, predictors= currPreds)
```


and take a quick look
```{r}
head(result.df)
```

Lets plot our Decrease in MSE for our top 10 predictors
```{r}
cut <- 10
ggplot(result.df %>%
       filter(Titles %in% Titles[1:cut]))+
    geom_segment(aes(x=1,xend=id,y=0,yend=0),size=.1,color="black")+
    geom_segment(aes(x=id,xend=id,y=0,yend=minMSE),color="blue")+
    geom_point(aes(x=id,y=minMSE),color="blue",size=1.5)+
    geom_segment(aes(x=id,xend=id,y=0,yend=diffMSE),color="red")+
    geom_point(aes(x=id,y=diffMSE),color="red",size=1.5)+
    scale_x_continuous(breaks=1:length(Titles),label=Titles)+
    coord_flip()+
    ggtitle("Subset Selection: MSE Decrease")
```

These are the top 10 variables that can be used to predict someone's age based on the attrition dataset. The blue part shows the overall reduction in MSE whilst the red part shows the difference in MSE with the use of each variable.

(something to note is that the plot is flipped)

We can now use those variables in our PCA to see how different ages cluster together

Lets filter our results dataframe so we are looking at our variables that gives us the lowest MSEs
```{r}
bestvars <- result.df[1:10,]
```


We put our best predictors to form our dataset for clusters and also take out ages from our original dataset.
```{r}
attri.df <- attri2[,bestvars$predictors]
ages <- attri2$Age

```
Take a look
```{r}
head(attri.df)
```

Attrition is non numerical which is going to give us issues later on so lets change that now to some 1s and 0s, if Yes = 1 if No =0. Also we will take out Age so it doesn't distort our clusters.

```{r}
attri.df <- data.frame(select_if(attri.df, is.numeric))
```

Take another look
```{r}
head(attri.df)
```

Okay now lets try out our PCA with the plot

```{r}
attri.pca <- prcomp(attri.df)
fviz_pca_biplot(attri.pca,repel=TRUE)
```
This is really intense and hard to understand, it seems like most of the variables fit into one dimension.

Lets pull off the PCA information
```{r}
rots <- attri.pca$rotation
```


Rotate into the new basis.
```{r}
attri.pca1<- data.matrix(attri.df) %*% rots
dim(attri.pca1)


```


The first three directions
```{r}
pca1 <- attri.pca1[,1]
pca2 <- attri.pca1[,2]
pca3 <- attri.pca1[,3]
```


##Add in K Means clustering to the Principal Components


2 clusters, this is arbitrary
```{r}
K <- 2
```


run 30 times, allow up to 30  iterations for convergence
```{r}
attri.km <- kmeans(attri.df,K,iter.max=20,nstart=30)

```


Our clusters
```{r}
attri.km$cluster

```


Add the clusters to the data frame
```{r}
attri.df$cluster.km<- attri.km$cluster
head(attri.df)

```


Look at one cluster
What ages land where?
```{r}

attri.df$age <- attrition$Age
filter(attri.df,cluster.km==1)%>%
    dplyr::select(age)


```


What are the cluster sizes?
```{r}
with(attri.df,table(cluster.km))

```


Build the rotated data frame
```{r}
attriCluster.df <- data.frame(pca1,pca2,cluster=factor(attri.df$cluster.km),name=ages)

```

plot the cluster.pcas on top of the PCA first two components
```{r}
ggplot(sample(attriCluster.df, (nrow(attriCluster.df)/4), replace = FALSE),aes(pca1,pca2,color=cluster))+
    geom_point(size=1)+
    geom_text_repel(aes(label=name),size=3)+
    ggtitle("Attrition Cluster Analsys and PCA")
```
We did a sample here because we saw how confusing the first plot was with the number of records, both clusters have a similiar number of ages. SO the plot does not say much. Lets take a look at the tables

```{r}
with(attri.df,table(ages, cluster.km))
```
Theres a pretty even distribution of ages between the clusters. So we probably need more clusters to seperate the clusters more. Also there are a few too many ages types so we may look into better categorizing those as well.


Lets find the optimum number of clusters 
```{r}
fviz_nbclust(attri.df,kmeans,method="wss")
```

From the graph it looks like the optimum is 6 so we will cluster at 6 and see what that gives us

Lets also better categorize ages so things make a bit more sense

```{r}
head(attri.df)
levels(factor(attri.df$age))
```


Better catgerorizing age so its easier to look at
```{r}
attri.df <- attri.df %>%
  mutate(age2 = ifelse(age <=20,"Teens",
                       ifelse(age <=30, "20s",
                              ifelse(age <= 40, "30s",
                                     ifelse(age<= 50, "40s",
                                            ifelse(age <= 60, "50s"))))))
```

Did it work
```{r}
levels(factor(attri.df$age2))
```

OKay now lets run everything and see our final clusters!!!
```{r}
K <- 6
finattri.df <- dplyr::select(attri.df, -age, -age2)
attri.km <- kmeans(finattri.df,K,iter.max=20,nstart=30)
attri.df$cluster.km<- attri.km$cluster
with(attri.df,table(cluster.km))
attriCluster.df <- data.frame(pca1,pca2,cluster=factor(attri.df$cluster.km),Age=factor(attri.df$age2))
ggplot(sample(attriCluster.df, (nrow(attriCluster.df)/4), replace = FALSE),aes(pca1,pca2,color=cluster))+
    geom_point(size=1)+
    geom_text_repel(aes(label=Age),size=3)+
    ggtitle("Attrition Cluster Analsys and PCA")
```

From the clusters it looks like age is not a very good variable for differentiating employees as there is a pretty even spread of ages between each cluster


The table below better exemplifies this
```{r}
with(attriCluster.df,table(Age, cluster))
```
So we can conclude form the data, in this case Age itself does not seperate employee groups, as you can be similar to any other employee regardless of your age.




Clean up data a little bit. Change variables to factors and remove variables that does not really help with the data. 
```{r}
attrition <- read.csv("~/Desktop/PROJECT-ADM-master/ADM/WA_Fn-UseC_-HR-Employee-Attrition.csv")

attrition$Education <- as.factor(attrition$Education)
attrition$EnvironmentSatisfaction <- as.factor(attrition$EnvironmentSatisfaction)
attrition$JobInvolvement <- as.factor(attrition$JobInvolvement)
attrition$JobLevel <- as.factor(attrition$JobLevel)
attrition$JobSatisfaction <- as.factor(attrition$JobSatisfaction)
attrition$PerformanceRating <- as.factor(attrition$PerformanceRating)
attrition$RelationshipSatisfaction <- as.factor(attrition$RelationshipSatisfaction)
attrition$StockOptionLevel <- as.factor(attrition$StockOptionLevel)
attrition$WorkLifeBalance <- as.factor(attrition$WorkLifeBalance)

attrition$EmployeeCount <- NULL
attrition$EmployeeNumber <- NULL
attrition$StandardHours <- NULL
attrition$Over18 <- NULL
attrition$MonthlyIncome <- NULL
attrition$MonthlyRate <- NULL

```

Split data into train and test train.
```{r}
n0 <- nrow(attrition)
train <- sample(1:n0,n0/2,rep=F)
train.df <- attrition[train,]
test.df <- attrition[-train,]
#names(train.df)
```

#First we do Decision Tree

Here, we are going to predict Atttrition using decision tree and see which variable is the most important in predicting whether or not someone will leave their job. 
```{r}
mod.tree <- tree(Attrition~., data=train.df,control=tree.control(nrow(attrition),mindev = 0.0001))
summary(mod.tree)
```

Let's see the tree. 
```{r}
plot(mod.tree)
text(mod.tree, pretty=0,cex=0.3)
```
Observation: Seems like some of the most important variable for Attrition are DistanceFromHome, Age, DailyRate. This tree is a little difficult to read, but let's make some predictions!

Predict train data. how well was our train data?
```{r}
##make some predictions with test data
preds.train<- predict(mod.tree,type="class")

##confusion matrix
with(train.df,table(Attrition,preds.train))

(err.train<- with(train.df,mean(Attrition!=preds.train)))
```
Conclusion: Our train data was pretty good with error rate of 0.05792904. Of course this might change everytime we run the tree. 

How about with our test data?
```{r}
##make some predictions with test data
preds.test<- predict(mod.tree,newdata=test.df,type="class")
##confusion matrix
with(test.df,table(Attrition,preds.test))

(err.test <- with(test.df,mean(Attrition!=preds.test)))
```
Conclusion: Not bad, I think maybe we can do better?


Here, we are gunna try with smaller mindev, how does err.test change?
```{r warning=FALSE}
mod.tree2 <- tree(Attrition~.,data=train.df,
                  control=tree.control(nrow(train.df),mindev=0.0000000))
(numLeaves <- sum(mod.tree2$frame$var =="<leaf>"))

numFolds <- 25
folds <- sample(1:numFolds,nrow(train.df),rep=TRUE)

err <- matrix(nrow=numLeaves,ncol=3)
errCV <- numeric(numFolds)
for(treeSize in numLeaves:2){
    print(treeSize)
    model.prune <- prune.tree(mod.tree2,best=treeSize)
    preds.train <- predict(model.prune,newdata=train.df,type="class")
    preds.test <- predict(model.prune,newdata=test.df,type="class")
    errTrain <- with(train.df,mean(Attrition!=preds.train))
    errTest <- with(test.df,mean(Attrition !=preds.test))
    ##Cross validate: 
    for(fold in 1:numFolds){
        attTrainTrain.df <- train.df[fold != folds,]
        attTrainTest.df <- train.df[fold == folds,]
        att.cv <- tree(Attrition~.,data=attTrainTrain.df,
                          control=tree.control(nrow(attTrainTrain.df),mindev=0.0000000))
        att.cv.prune <- prune.tree(att.cv,best=treeSize)
        preds <- predict(att.cv.prune,newdata=attTrainTest.df,type="class")
        errCV[fold] <- with(attTrainTest.df,mean(Attrition!= preds))
    }
    err[treeSize,] <- c(errTrain,errTest,mean(errCV))
}

data.frame(treeSize=2:numLeaves,
           train=err[-1,1],
           test=err[-1,2],           
           cv=err[-1,3]) %>%
    gather(type,val,train:cv) %>% 
    ggplot()+
    geom_point(aes(treeSize,val,color=type),size=1)+
    geom_line(aes(treeSize,val,color=type))+    
    scale_color_manual(values=c("red","blue","black"))

```
Seems like test error did not change at all. But mean cross-validation increase as treeSize increase which is good.

Let's prune our tree! Single Cross validation for optimal size
```{r}
mod.cv <- cv.tree(mod.tree,FUN=prune.misclass)
plot(mod.cv$size,mod.cv$dev)
plot(mod.cv)
```

What is our optimal tree size?
```{r}
(opt.size <- mod.cv$size[which(mod.cv$dev==min(mod.cv$dev))])
size <- rev(mod.cv$size)
(opt.size2 <- size[which.min(rev(mod.cv$dev))])
```

```{r}
par(mfrow=c(1,2))
plot(mod.cv$size,mod.cv$dev,type="b")
plot(mod.cv$k,mod.cv$dev,type="b")

```

Now let us prune this using the optimal size
```{r}
mod.prune <- prune.misclass(mod.tree,best=opt.size)
plot(mod.prune)
text(mod.prune,pretty=0,cex=0.5)

```
Observation: Seems like the important variable for employees staying at their company is HourlyRate, RelationshipSatisfacion, YearsAtCompany, and EnvironmentSatisfication. 

Predict our prune model using the testing data. What do we find?
```{r}
mod.pred.prune <- predict(mod.prune,test.df,type="class")
summary(mod.pred.prune)

##confusion matrix
with(test.df,table(Attrition,mod.pred.prune))

(err.prune.test<- with(test.df,mean(Attrition!=mod.pred.prune)))
```


##compare error rates from the tree test and train errors and the pruned tree

```{r}
c(err.train,err.test,err.prune.test)
```
Looks like we did a better job finding the most important variable for predicting attrition when we pruned our tree and found the best size!! Nice!


Let's predict our prune tree with out testing data set. How does it look?
```{r}
probs.prune <- predict(mod.prune,newdata=test.df)
hist(probs.prune[,1],breaks=25)
```


##Random Forest 

Here we are going to perform some out of bag error with Random Forest!! What does random forest say the most important variable for predicting Attrition? How well? 

```{r}
p <- ncol(train.df)-1
mod.rf <- randomForest(Attrition~., data = train.df, ntree = 500, mtry = p,importance = TRUE)
mod.rf

```


Now, let's make a prediction using our testing data and see how well we did. 
```{r}
## Make some predictions on the test data
preds.bag <- predict(mod.rf,newdata=test.df,ntree=500)

## Confusion matrix
with(test.df,table(Attrition,preds.bag))

(err.bag <- with(test.df,mean(Attrition!=preds.bag)))

```

Let's compare the error rates from our decision tree, prune tree, and the out of bag error
```{r}
c(err.test,err.prune.test,err.bag)
```
Looks like randomForest is winning. Doing really well. 

```{r}
plot(mod.rf)
```
As we can see from above black line shows the error rate with OOB. It decreases as the number of trees increases and eventually stablized to a value.

```{r}
importance(mod.rf)
```

Below is a plot of the variable of importance for our MeanDecreaseGini,MeanDecreaseAccuracy. 
It looks like the biggest and important variable for the mean decrease accuracy is overtime followed by MonthlyIncome then Age and JobRole. Next for the MeanDecreaseGini MonthlyIncome, DailyRate, Overtime, Age. 
```{r}
varImpPlot(mod.rf,
           sort=TRUE,n.var=10,main="Top 10 Variable of Importance")
```

Let's do Adaboost!
```{r}
require(dplyr)
attrition <- attrition %>%
      mutate(Attrition = ifelse(Attrition== "No",0,1))
 
attrition$Education <- as.factor(attrition$Education)
attrition$EnvironmentSatisfaction <- as.factor(attrition$EnvironmentSatisfaction)
attrition$JobInvolvement <- as.factor(attrition$JobInvolvement)
attrition$JobLevel <- as.factor(attrition$JobLevel)
attrition$JobSatisfaction <- as.factor(attrition$JobSatisfaction)
attrition$PerformanceRating <- as.factor(attrition$PerformanceRating)
attrition$RelationshipSatisfaction <- as.factor(attrition$RelationshipSatisfaction)
attrition$StockOptionLevel <- as.factor(attrition$StockOptionLevel)
attrition$WorkLifeBalance <- as.factor(attrition$WorkLifeBalance)
attrition$EducationField  <- as.factor(attrition$EducationField)
attrition$Gender <- as.factor(attrition$Gender)
attrition$JobRole<- as.factor(attrition$JobRole)
attrition$MaritalStatus<- as.factor(attrition$MaritalStatus)
attrition$OverTime<- as.factor(attrition$OverTime)
attrition$BusinessTravel <- as.factor(attrition$BusinessTravel)
attrition$Department <- as.factor(attrition$Department)

attrition$EmployeeCount <- NULL
attrition$EmployeeNumber <- NULL
attrition$StandardHours <- NULL
attrition$Over18 <- NULL
attrition$MonthlyIncome <- NULL
attrition$MonthlyRate <- NULL

```

```{r}
library(gbm)
numTrees <- 100
depth <- 7
lambda <- .1
att.gbm <- gbm(Attrition ~ ., data=attrition,
                n.trees=numTrees,
                distribution="adaboost",
                interaction.depth = depth,
                shrinkage=lambda)

```


```{r}
probs <- predict(att.gbm,newdata=attrition,n.trees=100,type="response")
##convert to a prediction.
preds <-  probs > 0.5

## How does it look?
with(attrition,table(Attrition,preds))
(err.gbm <- with(attrition,mean(Attrition != preds)))
```
 
A quick look at the error as a function of the number of trees.
```{r}
errTree <- numeric(numTrees)
for(n in 1:numTrees){
  probs <- predict(att.gbm,newdata=attrition,n.trees=n,type="response")
  preds <-  probs > 0.5
  errTree[n] <- with(attrition,mean(Attrition != preds))  
}


data.frame(trees=1:numTrees,err=errTree) %>% 
  ggplot()+
  geom_point(aes(trees,err))

```
Looks good! 

Let's compare all of our error rates together! err.test is decision trees, err.prune.test is prune tree, err.bag is random forest, err.gbm is adaboost. 
```{r}
c(err.test,err.prune.test,err.bag,err.gbm)

```


Let's look at the summary below. It looks like the JobRole is the top most important variable for predicting Attrition based on AdaBoost, Second is Overtime, Age, and DistanceFromHome are the other important variables. 
```{r}
summary(att.gbm)
```


The error rate for all linear, polynomial and radial kernel seems to be 1


```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(glmnet)
library(FNN)
library(randomForest)
library(mlbench)
library(devtools)
install_github("seniorburito/ensemble")
```
# Ensembling

Here we are trying to see if the numerical data in this dataset can help predict hourly pay rate of the employees, without considering categorical data such as the department, education field and etc. The numerical data include walking distance from home, self-reported job satisfcation on scale of 5, employee performance rating, and etc. 
```{r}
# dat <- as.data.frame(dat)
# View(dat)


attri.df <- attrition[,-2]
attri.df <- data.frame(select_if(attri.df, is.numeric))
#attri.df <- data.frame(scale(attri.df))
#View(attri.df)

train_inds <- sample(1:nrow(attri.df), floor(0.8*nrow(attri.df)), rep=F)
train <- attri.df[train_inds,]
test <- attri.df[-train_inds,]

X_train <- dplyr::select(train, -HourlyRate)
X_test <- dplyr::select(test, -HourlyRate)
y_train <- train$HourlyRate
y_test <- test$HourlyRate

pred <- RooEnsemble::average_pred(X_train, y_train, X_test)
err <- mean((pred-y_test)^2)
err
```
We are using ensemble function that considers KNN, Random forest, LASSO, Ridge and linear models. The predictions from all the models are averaged. 

The Mean Square Error for this is really bad. The hourly rate ranges around 90s and the mean square error is 400. If we look at how each model performed we see: 

```{r}
error_plot <- function(X_train, y_train, X_test, y_test, rfntree, KNNk, rfmtry, doKNN){
  if(missing(rfntree)){
    rfntree = 100
  }
  if(missing(KNNk)){
    KNNk = 5
  }
  if(missing(rfmtry)){
    rfmtry = 6
  }
  if(missing(doKNN)){
    doKNN = TRUE
  }
  # Creating ridge and lasso sets
  X_train_glmnet <- model.matrix(~.-1, data=X_train)
  X_test_glmnet <- model.matrix(~.-1, data=X_test)
  # LM
  fit_lm <- glmnet(X_train_glmnet, y_train, alpha=0, lambda=c(0))
  pred_lm <- predict(fit_lm, X_test_glmnet)[,1]
  error_lm <- mean((pred_lm-y_test)^2)
  # KNN
  pred_knn <- knn.reg(X_train, test=X_test, y=y_train, k=KNNk)$pred
  error_knn <- mean((pred_knn-y_test)^2)
  # RF
  fit_rf <- randomForest(x=X_train, y=y_train, ntree=rfntree, mtry = rfmtry)
  pred_rf <- predict(fit_rf, X_test)
  error_rf <- mean((pred_rf-y_test)^2)
  # Ridge
  fit_ridge <- cv.glmnet(X_train_glmnet, y_train, alpha=0)
  pred_ridge <- predict(fit_ridge, X_test_glmnet, s='lambda.min')[,1]
  error_ridge <- mean((pred_ridge-y_test)^2)
  # LASSO
  fit_lasso <- cv.glmnet(X_train_glmnet, y_train, alpha=1)
  pred_lasso <- predict(fit_lasso, X_test_glmnet, s='lambda.min')[,1]
  error_lasso <- mean((pred_lasso-y_test)^2)

  # Computing average
  if(doKNN){
    avg_preds <- rowMeans(matrix(c(pred_lm, pred_knn, pred_rf, pred_ridge, pred_lasso),ncol=5))
    error_avg <- mean((avg_preds-y_test)^2)
    results <- data.frame(Model=c('LM','KNN','RF','Ridge','LASSO','Average'),
                          MSE=c(error_lm, error_knn, error_rf, error_ridge, error_lasso, error_avg))
    ggplot(results, aes(x=fct_reorder(Model, MSE), y=MSE))+
      geom_col(fill='red')+
      ggtitle('Comparing model performance')
  }
  else{
    avg_preds <- rowMeans(matrix(c(pred_lm, pred_rf, pred_ridge, pred_lasso),ncol=4))
    error_avg <- mean((avg_preds-y_test)^2)
    results <- data.frame(Model=c('LM','RF','Ridge','LASSO','Average'),
                          MSE=c(error_lm, error_rf, error_ridge, error_lasso, error_avg))
    ggplot(results, aes(x=fct_reorder(Model, MSE), y=MSE))+
      geom_col(fill='red')+
      ggtitle('Comparing model performance')
  }
}

error_plot(X_train, y_train, X_test, y_test, doKNN = TRUE)
```
Unsurprisingly, KNN is worse than the other models when considering so many predictors. We can remove KNN from the model, and see how that changes the MSE:

```{r}
pred <- RooEnsemble::average_pred(X_train, y_train, X_test, doKNN = FALSE)
err <- mean((pred-y_test)^2)
err
``` 
This is better, but not by a lot. The MSE is still high. What if we cross-validate this? Implementing this into the ensemble package was not successful, so we are just going to hard code this: 

```{r}

#LM
fit_lm <- lm(HourlyRate~., data=train)
pred_lm <- predict(fit_lm, test)
error_lm <- mean((pred_lm-y_test)^2)

numFolds<-15
N<-nrow(train)
folds<-sample(1:numFolds,N,replace=TRUE)
  
mseKFold<-numeric(numFolds)
  for(fold in 1:numFolds){
    X_train_glmnet  <- train[folds != fold,]
    X_test_glmnet   <- train[folds == fold,]
    fit_lm <- lm(HourlyRate~., data=X_train_glmnet)
    pred_lm <- predict(fit_lm, X_test_glmnet) 
    mseKFold[fold] <- with(X_test_glmnet,mean((HourlyRate-pred_lm)^2))
  }
(mse.kfold <- mean(mseKFold))


# RandomForest
fit_rf <- randomForest(HourlyRate~., data=train, ntree=200)
pred_rf <- predict(fit_rf, test)
error_rf <- mean((pred_rf-y_test)^2)

numFolds<-5
N<-nrow(train)
folds<-sample(1:numFolds,N,replace=TRUE)
  
mseKFold.RF<-numeric(numFolds)
  for(fold in 1:numFolds){
    X_train_glmnet  <- train[folds != fold,]
    X_test_glmnet   <- train[folds == fold,]
    fit_rf <- randomForest(HourlyRate~., data=X_train_glmnet, ntree=200)
    pred_rf <- predict(fit_rf, X_test_glmnet)
    mseKFold.RF[fold] <- with(X_test_glmnet,mean((HourlyRate-pred_rf)^2))
  }
(mse.kfold <- mean(mseKFold.RF))


# Ridge
fit_ridge <- cv.glmnet(model.matrix(HourlyRate~.-1, data=train), y_train, alpha=0)
pred_ridge <- predict(fit_ridge, model.matrix(HourlyRate~.-1, data=test), s='lambda.min')[,1]
error_ridge <- mean((pred_ridge-y_test)^2)


numFolds<-5
N<-nrow(train)
folds<-sample(1:numFolds,N,replace=TRUE)
  
mseKFold.R<-numeric(numFolds)
  for(fold in 1:numFolds){
    X_train_glmnet  <- train[folds != fold,]
    X_test_glmnet   <- train[folds == fold,]
    fit_ridge <- cv.glmnet(model.matrix(HourlyRate~.-1, data=train), y_train, alpha=0)
   pred_ridge <- predict(fit_ridge, model.matrix(HourlyRate~.-1, data=test), s='lambda.min')[,1]
    mseKFold.RF[fold] <- with(X_test_glmnet,mean((pred_ridge-y_test)^2))
  }
(mse.kfold <- mean(mseKFold.R))


# LASSO
fit_lasso <- cv.glmnet(model.matrix(HourlyRate~.-1, data=train), y_train, alpha=1)
pred_lasso <- predict(fit_lasso, model.matrix(HourlyRate~.-1, data=test), s='lambda.min')[,1]
error_lasso <- mean((pred_lasso-y_test)^2)

numFolds<-5
N<-nrow(train)
folds<-sample(1:numFolds,N,replace=TRUE)
  
mseKFold.R<-numeric(numFolds)
  for(fold in 1:numFolds){
    X_train_glmnet  <- train[folds != fold,]
    X_test_glmnet   <- train[folds == fold,]
    fit_lasso <- cv.glmnet(model.matrix(HourlyRate~.-1, data=train), y_train, alpha=1)
   pred_lasso <- predict(fit_lasso, model.matrix(HourlyRate~.-1, data=test), s='lambda.min')[,1]
    mseKFold.RF[fold] <- with(X_test_glmnet,mean((pred_lasso-y_test)^2))
  }
(mse.kfold <- mean(mseKFold.R))

```
The results do not make a lot of sense. For linear model and random forest, the mean square error is higher, which should be lower, but for Ridge and Lasso, the mse is way too low. 

Overall, such high MSE suggests that these are not good predictors for hourly rate of employees.


```{r}
attrition <- read.csv("~/Desktop/PROJECT-ADM-master/ADM/WA_Fn-UseC_-HR-Employee-Attrition.csv")
data.df <- mutate(attrition,
               Attrition=ifelse(Attrition=="Yes",1, ifelse(Attrition=="No",0,2)))
head(data.df)
data.df <- attrition[ -c(9,10,22,27)]
```

```{r}
# Separate data into train and test set
nr <- nrow(data.df)
set.seed(330)
train <- sample(1:nr, nr/2, rep=F)
train.df <- data.df[train,]
test.df <- data.df[-train,]
sampleSize <- nrow(data.df)
```


#SVM with Linear Kernel
```{r}
library(e1071)
theCost <- .05
svm.linear <- svm(Attrition~.,
               data=train.df,
               kernel="linear",
               cost=theCost,
               scale=F)
# Make prediction using our linear kernel SVM on the test data
pred.linear <- predict(svm.linear, newdata = test.df)
# Error Rate
(error.svm.linear <- with(test.df, mean(Attrition != pred.linear)))
```
Error rate is 1
#SVM with Polynomial Kernel
```{r}
# Tune our SVM with a Polynomial Kernel to obtain the best parameters
tune.poly <- tune(svm, Attrition~., 
                    data = train.df,
                    kernel = "polynomial",
                    scale = T,
                    ranges=list(cost=10^seq(-3,3,length=10), degree=seq(1,5)),
                    tunecontrol=tune.control(cross=3))
C <- tune.poly$best.parameters[1]
D <- tune.poly$best.parameters[2]
# Re-run SVM with optimized parameters
svm.poly <- svm(Attrition~.,
                  data = train.df, 
                  kernel = "polynomial",
                  scale = T,
                  cost = C,
                  degree = D,
                  decision.values=T)
# Make prediction using our polynomia kernel SVM on the test data
pred.poly <- predict(svm.poly, newdata = test.df)
# Error Rate
(error.svm.poly <-  with(test.df, mean(Attrition != pred.poly)))
```
#SVM with Radial Kernel
```{r}
# Tune our SVM with a Linear Kernel to obtain the best parameters
tune.radial <- tune(svm, Attrition~., 
                  data = train.df,
                  kernel = "radial",
                  scale = T,
                  ranges=list(cost=10^seq(-3,3,length=10), gamma=10^seq(-3,3)),
                  tunecontrol=tune.control(cross=3))
C <- tune.radial$best.parameters[1]
G <- tune.radial$best.parameters[2]
# Re-run with new parameters
svm.radial <- svm(Attrition~.,
                data = train.df, 
                kernel = "radial",
                scale = T,
                cost = C,
                gamma = G,
                decision.values=T)
# Predict with Radial Kernel
pred.radial <- predict(svm.radial, newdata = test.df)
# Error Rate
(error.svm.radial <- with(test.df, mean(Attrition != pred.radial)))
```





Conclusion: In terms of clustering using PSA we can conclude from the data, in this case Age itself does not seperate employee groups, as you can be similar to any other employee regardless of your age. In terms of assessing with algorithm is best in predicting Attrition, in this case, we used Decision Trees, Pruned Tree, RandomForest, AdaBoost. AdaBoost is one of the best algorithm to make a prediction on Attrition with an error rate of 0.0748 and RandomForest is the second best in predicting. 

Using svm, the results were inconclusive. The error rates, for all types of svm i.e linear, polynomial and radial was one which could be because of the parameters we set.

Using ensemble, the resuls showed that numerical predictors from this dataset are not a good at predicting hourly pay rate. 
```{r}
c(err.test,err.prune.test,err.bag,err.gbm)
```