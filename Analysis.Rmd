---
title: "Random forest and Decision Trees"
output: html_document
---

```{r}
suppressMessages(library(lattice))
suppressMessages(library(dplyr))
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(MASS))
suppressMessages(library(tree))
suppressMessages(library(mosaic))
suppressMessages(library(randomForest))
```

## Exploratory Data Analysis

```{r}
#Lets take a look at the data
attrition = read.csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
```

```{r}
#Check for missing values
#is.na(attrition)
ggplot(attrition, aes(x = MonthlyIncome)) + geom_histogram()
```
Histogram shows the distribution of monthly income in our dataset. The histogram is left skewed therefore we see that monthly income 

```{r}
# Age against Monthly income per education field
Age = attrition$Age
MonthlyIncome = attrition$MonthlyIncome
EducatioField = attrition$EducationField

qplot(Age, log(MonthlyIncome), data = attrition, color = EducationField)
```
The graphs above shows a positive relationship between age and monthly income across different kinds of educational fields.

```{r}
# Years since last promotion vs Years with current manager
qplot(YearsSinceLastPromotion, YearsWithCurrManager, data = attrition)
```
There does not seem to be any significant relationship between years since last promotion and years with current manager due to the random distribution shown in the plot

```{r}
#Attrition vs Number of companies worked

hase = ggplot(attrition, aes(x = Attrition, y = NumCompaniesWorked)) +
  geom_boxplot()
hase
```
The boxplot above shows that most employees lost to attrition have worked fewer companies than those who are not.

```{r}
#Attrition vs Distance from Home

hose = ggplot(attrition, aes(x = Attrition, y = DistanceFromHome)) + geom_boxplot()
hose
```
The graphs above suggests that the greater the distance from home the more likely it is for an employee to be lost through attrition.

```{r}
ggplot(attrition,aes(DistanceFromHome,fill=Attrition))+
  geom_density(alpha=0.5)+
  theme_few()+
  theme(legend.position="bottom",plot.title=element_text(hjust=0.5,size=16))+
  labs(x="Distance from Home",title="Attrition and Distance from Home")
```

```{r}
#Attition vs Daily Rate/Distance From Home/ Houtly Rate
old.par = par()
par(mfrow = c(1,3))
with (attrition, {
  boxplot(attrition$DailyRate ~ attrition$Attrition, xlab = "Attrition by Daily Rate")
  boxplot(attrition$HourlyRate ~ attrition$Attrition, xlab = "Attrition by Hourly Rate")
  boxplot(attrition$MonthlyRate ~ attrition$Attrition, xlab = "Attrition by Monthly Rate")
}
)
par(old.par)
```

From the boxplots above, it seems attrition is most significantly affected by the daily rate as those who faced attrition had a significantly lower mean for daily rate as opposed to those who did not. As for attrition by hourly rate and by monthly rate, there does not seem to be any significant difference in means.

```{r}
#Work Life Balance vs Status
gaal = ggplot(attrition, aes(x = MaritalStatus, y = WorkLifeBalance)) + geom_boxplot()
gaal
```

The boxplot suggests that Work and Life Balance are irrespective of marital status as the means across the three different marital groups are the same

```{r}
#Job role vs percent salary hike
Haha = favstats(~attrition$PercentSalaryHike|attrition$JobRole)
Haha

bwplot(attrition$PercentSalaryHike ~ attrition$JobRole, data=attrition)

#attrition vs percent salary hike
Hehe = favstats(~attrition$PercentSalaryHike|attrition$Attrition)
Hehe
bwplot(attrition$PercentSalaryHike ~ attrition$Attrition, data=attrition)

#Gender by Montly Income
bwplot(attrition$MonthlyIncome ~ attrition$Gender, data=attrition)
```

Based on the tables above, Percent Salary Hike is independent of Job Role and is not a major factor in determining attrition as the means are almost the same between those employees lost to attrition and those not. It is also interesting from the Monthly income by gender boxplot that both men and women seem to be earning roughly the same amount which is usually not the case.

```{r}
Perfomance = attrition$PerformanceRating
MonthlyIncome = attrition$MonthlyIncome
Monthly.rate = attrition$MonthlyRate

qplot(Monthly.rate, MonthlyIncome, data = attrition, color = Perfomance)

old.par = par()
# create a grid of one row and two columns
par(mfrow = c(1,2))
with(c,{ggplot(c,aes(y = MonthlyIncome, x = log(Monthly.rate))) +
  geom_point()
  p1 +
  geom_point(aes(color = "Perfomance == 3"))
  ggplot(c,aes(y = MonthlyIncome, x = log(Monthly.rate))) +
  geom_point()
  p1 +
  geom_point(aes(color = "Perfomance == 4"))
}
)

c = data.frame(Monthly.rate, MonthlyIncome, Perfomance)
ggplot(c,
       aes(y = MonthlyIncome, x = log(Monthly.rate))) +
  geom_point()

p1 = ggplot(c, aes(x = log(Monthly.rate), y = MonthlyIncome))

p1 +
  geom_point(aes(color = Perfomance))

c$pred.SC = predict(lm(MonthlyIncome ~ log(Monthly.rate), data = c))

p1 + geom_point(aes(color = Perfomance)) +
  geom_line(aes(y = c$pred.SC))
```

From the above scatterplot, we can see that there is no linear relationship between monthly income and monthly rate

```{r}
counts = table(attrition$Attrition, attrition$BusinessTravel)
barplot(counts, main = "Attrition vs Business Travel", xlab="Number of attrition", col=c("darkblue","red"),
 	legend = rownames(counts))

S=favstats(~attrition$JobInvolvement|attrition$Attrition)
S

T=favstats(~attrition$JobSatisfaction|attrition$Attrition)
T
```


##Regressions

**Linear**

```{r}
names(attrition)
mod.lin <- lm(Age~ DailyRate+DistanceFromHome+JobRole+YearsAtCompany,data=attrition)
summary(mod.lin)
```


**Logistic**

```{r}
mod.log <- glm(Age+TotalWorkingYears+MonthlyRate~Attrition,data=attrition)
summary(mod.log)
```


**Simple plot using train data: Here I am interested in looking at how MonthlyIncome is affected by Age in terms of the TotalWorkingYears. Later, we are gunna try to predict the relationship of this and try to cluster. We can define the number of cluster that we want in terms of the TotalWorkingYears. We are gunna try to cluster this in a moment.**

```{r}
ggplot(attrition)+
  geom_point(aes(Age,MonthlyIncome,color=factor(TotalWorkingYears)))
```

##Let's just do a bit of data clean up. We can try to put this in four categories. 
```{r}
attrition <- data.frame(attrition)

attrition$Education <- as.factor(attrition$Education)
attrition$EnvironmentSatisfaction <- as.factor(attrition$EnvironmentSatisfaction)
attrition$JobInvolvement <- as.factor(attrition$JobInvolvement)
attrition$JobLevel <- as.factor(attrition$JobLevel)
attrition$JobSatisfaction <- as.factor(attrition$JobSatisfaction)
attrition$PerformanceRating <- as.factor(attrition$PerformanceRating)
attrition$RelationshipSatisfaction <- as.factor(attrition$RelationshipSatisfaction)
attrition$StockOptionLevel <- as.factor(attrition$StockOptionLevel)
attrition$WorkLifeBalance <- as.factor(attrition$WorkLifeBalance)

attrition$EmployeeCount <- NULL
attrition$EmployeeNumber <- NULL
attrition$StandardHours <- NULL
attrition$Over18 <- NULL

summary(attrition)
```

Split data into train and test train.
```{r}
n0 <- nrow(attrition)
train <- sample(1:n0,n0/2,rep=F)
train.df <- attrition[train,]
test.df <- attrition[-train,]
#names(train.df)
```

Decision Tree

```{r}
mod.tree <- tree(Attrition~., data=train.df,control=tree.control(nrow(attrition),mindev = 0.01))
summary(mod.tree)
```

```{r}
plot(mod.tree)
text(mod.tree, pretty=0,cex=0.5)
```
predict train data. how well was our train data?
```{r}
##make some predictions with test data
preds.train<- predict(mod.tree,type="class")

##confusion matrix
with(train.df,table(Attrition,preds.train))

(err.train<- with(train.df,mean(Attrition!=preds.train)))
```

```{r}
##make some predictions with test data
preds.test<- predict(mod.tree1,newdata=test.df,type="class")

##confusion matrix
with(test.df,table(Attrition,preds.test))

(err.test <- with(test.df,mean(Attrition!=preds.test)))
```

Try with smaller mindev, how does err.test change?

```{r}
mod.tree <- tree(Attrition~.,data=train.df,
                  control=tree.control(nrow(train.df),mindev=0.000))
(numLeaves <- sum(mod.tree$frame$var =="<leaf>"))

numFolds <- 10
folds <- sample(1:numFolds,nrow(train.df),rep=T)

err <- matrix(nrow=numLeaves,ncol=3)
errCV <- numeric(numFolds)
for(treeSize in numLeaves:2){
    print(treeSize)
    model.prune <- prune.tree(mod.tree,best=treeSize)
    preds.train <- predict(model.prune,newdata=train.df,type="class")
    preds.ttest <- predict(model.prune,newdata=test.df,type="class")
    errTrain <- with(train.df,mean(Attrition!=preds.train))
    errTest <- with(test.df,mean(Attrition !=preds.test))
    ##Cross validate: 
    for(fold in 1:numFolds){
        attTrainTrain.df <- train.df[fold != folds,]
        attTrainTest.df <- train.df[fold == folds,]
        att.cv <- tree(Attrition~.,data=attTrainTrain.df,
                          control=tree.control(nrow(attTrainTrain.df),mindev=0.000))
        att.cv.prune <- prune.tree(att.cv,best=treeSize)
        preds <- predict(att.cv.prune,newdata=attTrainTest.df,type="class")
        errCV[fold] <- with(attTrainTest.df,mean(Attrition!=preds))
    }
    err[treeSize,] <- c(errTrain,errTest,mean(errCV))
}


data.frame(treeSize=2:numLeaves,
           train=err[-1,1],
           test=err[-1,2],           
           cv=err[-1,3]) %>%
    gather(type,val,train:cv) %>% 
    ggplot()+
    geom_point(aes(treeSize,val,color=type),size=1)+
    geom_line(aes(treeSize,val,color=type))+    
    scale_color_manual(values=c("red","blue","black"))

```

Let's prune our tree! Cross validate:
```{r}
mod.cv <- cv.tree(mod.tree,FUN=prune.misclass)
plot(mod.cv$size,mod.cv$dev)
```

What is our optimal tree size?
```{r}
(opt.size <- mod.cv$size[which(mod.cv$dev==min(mod.cv$dev))])
```

```{r}
par(mfrow=c(1,2))
plot(mod.cv$size,mod.cv$dev,type="b")
plot(mod.cv$k,mod.cv$dev,type="b")

```

Now let us prune this using the optimal size
```{r}
mod.prune <- prune.misclass(mod.tree,best=opt.size)
plot(mod.prune)
text(mod.prune,pretty=0,cex=0.5)

```
Observation: 







Predict our prune model using the testing data. What do we find?
```{r}
mod.predA <- predict(mod.prune,test.df,type="class")
summary(mod.predA)

##confusion matrix
with(test.df,table(Attrition,mod.predA))

(err.prune<- with(test.df,mean(Attrition!=mod.predA)))
```

##compare error rates

```{r}
c(err.tree,err.prune)
```
Looks like we did a better job finding the most important variable for predicting attrition when we pruned our tree and found the best size. 


##Random Forest 


Here we are going to perform some out
```{r}
mod.rf <- randomForest(Attrition~., data = train.df, ntree = 400, mtry = 6,importance = TRUE)
mod.rf
```

```{r}
## Make some predictions on the test data
preds.bag <- predict(mod.rf,newdata=test.df)


## Confusion matrix

with(test.df,table(Attrition,preds.bag))

(err.bag <- with(test.df,mean(Attrition!=preds.bag)))

```

```{r}
plot(mod.rf)

```

```{r}
importance(mod.rf)
```

```{r}
varImpPlot(mod.rf)
```

```{r}
gg.tree2 <- ggplot(attrition)+
    geom_tile(aes(Attrition,Age,fill=pred.tree))+
    scale_fill_gradient2(low="blue",midpoint=2.5,high="red")+
    ggtitle("Predicted Regions")+
    guides(fill=FALSE)
gg.tree2


```
Start with LDA using all predictors
```{r}
names(attrition)
mod.lda <- lda(Attrition~Age+DailyRate+Department+DistanceFromHome+MonthlyIncome+PercentSalaryHike+TotalWorkingYears+YearsSinceLastPromotion, data=train.df)
#mod.lda
pred.lda <- predict(mod.lda,newdata=test.df)
test.df <- test.df %>%
  mutate(pred=pred.lda$class)
##how good does it work
with(test.df,table(Attrition,pred))
```
Working on scaling the values

attrition<- mutate(attrition,
                 NewTotYears=ifelse(TotalWorkingYears <= 7 & TotalWorkingYears >=0, "Beginning",
                                    ifelse(TotalWorkingYears <= 15 & TotalWorkingYears >7, "Novice",
                                           ifelse(TotalWorkingYears <= 25 & TotalWorkingYears >15,"Intermediate",
                                                  ifelse(TotalWorkingYears <= 40 & TotalWorkingYears >25,"Expert","None")))))

attrition <- attrition %>% mutate(NewTotYears=as.factor(NewTotYears))

```{r}
#View(test.df)
Wvec <- mod.lda$scaling
val <- (dim(Wvec))
val <- as.numeric(val)
X <- data.matrix(test.df[,1:val])
sum(Wvec[,1]^2)
(s1 <- sum(Wvec[,1]^2))
(s2 <- sum(Wvec[,2]^2))
(Wvec[,1] <- Wvec[,1]/s1)
(Wvec[,2] <- Wvec[,2]/s2)
X.trans <- X %*% Wvec
dim(X.trans)
Wvec
```

```{r}
testTrans.df <- data.frame(TotalWorkingYears=factor(test.df$TotalWorkingYears),LDA1=X.trans[,1],LDA2=X.trans[,2])
ggplot(testTrans.df,aes(LDA1,LDA2,color=TotalWorkingYears))+
  geom_point()
```

Random Forest
```{r}



```


